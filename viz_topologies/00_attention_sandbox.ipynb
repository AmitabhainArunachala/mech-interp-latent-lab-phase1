{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention Sandbox - Notebook 00\n",
        "\n",
        "**Purpose:** Learn to capture and visualize attention patterns from a transformer model.\n",
        "\n",
        "**Goal:** Load GPT-2 small, run a simple prompt, extract attention from one layer, and visualize it as a heatmap.\n",
        "\n",
        "**MI Concept:** Understanding how attention heads distribute information across tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup and Imports\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"✓ Imports loaded\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Load GPT-2 Small Model\n",
        "model_name = \"gpt2\"  # GPT-2 small (124M parameters)\n",
        "\n",
        "print(f\"Loading {model_name}...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, output_attentions=True)\n",
        "\n",
        "# Set to eval mode\n",
        "model.eval()\n",
        "\n",
        "print(f\"✓ Model loaded: {model_name}\")\n",
        "print(f\"  Layers: {model.config.n_layer}\")\n",
        "print(f\"  Heads: {model.config.n_head}\")\n",
        "print(f\"  Hidden size: {model.config.n_embd}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Prepare a Simple Prompt\n",
        "prompt = \"The transformer model processes tokens through attention mechanisms.\"\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "# Get token strings for visualization\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "print(f\"\\nTokens ({len(tokens)}):\")\n",
        "for i, token in enumerate(tokens):\n",
        "    print(f\"  {i:2d}: {token}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Run Model and Capture Attention\n",
        "# We'll extract attention from a middle layer (layer 6 out of 12)\n",
        "\n",
        "target_layer = 6  # Middle layer\n",
        "target_head = 0   # First head (we can explore others later)\n",
        "\n",
        "print(f\"Running model forward pass...\")\n",
        "print(f\"Target: Layer {target_layer}, Head {target_head}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, output_attentions=True)\n",
        "\n",
        "# Extract attention: outputs.attentions is a tuple of (batch, layer, head, seq, seq) tensors\n",
        "# Shape: (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
        "attentions = outputs.attentions\n",
        "\n",
        "# Get attention for our target layer and head\n",
        "attention_matrix = attentions[target_layer][0, target_head, :, :].numpy()\n",
        "\n",
        "print(f\"\\n✓ Attention captured\")\n",
        "print(f\"  Shape: {attention_matrix.shape}\")\n",
        "print(f\"  Min: {attention_matrix.min():.4f}\")\n",
        "print(f\"  Max: {attention_matrix.max():.4f}\")\n",
        "print(f\"  Sum per row (should be ~1.0): {attention_matrix.sum(axis=1)[:3]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Visualize Attention as Heatmap\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "# Create heatmap\n",
        "im = ax.imshow(attention_matrix, cmap='Blues', aspect='auto', vmin=0, vmax=attention_matrix.max())\n",
        "\n",
        "# Set labels\n",
        "ax.set_xticks(range(len(tokens)))\n",
        "ax.set_yticks(range(len(tokens)))\n",
        "ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
        "ax.set_yticklabels(tokens)\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(im, ax=ax)\n",
        "cbar.set_label('Attention Weight', rotation=270, labelpad=20)\n",
        "\n",
        "# Labels and title\n",
        "ax.set_xlabel('Key Position (attended TO)', fontsize=12)\n",
        "ax.set_ylabel('Query Position (attended FROM)', fontsize=12)\n",
        "ax.set_title(f'Attention Pattern: Layer {target_layer}, Head {target_head}\\n\"{prompt[:50]}...\"', \n",
        "             fontsize=14, pad=20)\n",
        "\n",
        "# Add grid for readability\n",
        "ax.grid(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Heatmap displayed\")\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Rows = query positions (where attention is FROM)\")\n",
        "print(\"  - Columns = key positions (where attention is TO)\")\n",
        "print(\"  - Brightness = attention strength\")\n",
        "print(\"  - Each row sums to ~1.0 (softmax normalization)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations & Next Steps\n",
        "\n",
        "**What to notice:**\n",
        "- Which tokens attend to which other tokens?\n",
        "- Is there a diagonal pattern (attending to previous tokens)?\n",
        "- Are there specific tokens that receive high attention?\n",
        "- How does this differ from what you might expect?\n",
        "\n",
        "**Experiments to try:**\n",
        "1. Change `target_layer` to see how attention evolves across layers\n",
        "2. Change `target_head` to see different attention patterns\n",
        "3. Try different prompts (longer, shorter, different topics)\n",
        "4. Compare early vs late layers\n",
        "\n",
        "**Next notebook:** Residual Stream Explorer (01)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
