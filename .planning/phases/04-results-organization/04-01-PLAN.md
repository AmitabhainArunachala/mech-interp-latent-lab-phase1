# Plan 04-01: Audit All Result Directories

## Objective
Audit all 98+ result runs to identify paper-worthy vs exploratory/failed runs.

## Context
- 98 runs with summary.json across multiple locations
- Current structure is organic/ad-hoc from research phase
- Need to identify which runs contain publication-quality data

## Criteria for Canonical Results

### Paper-Worthy (canonical)
1. **rv_l27_causal_validation** - Core finding: R_V contraction is causal (p < 10⁻²²)
2. **confound_validation** - Controls ruling out confounds
3. **random_direction_control** - Random direction doesn't produce effect
4. **mlp_*_validation** - MLP ablation studies (necessity/sufficiency)
5. Any run with n ≥ 45, p < 0.001, clear effect direction

### Methodology (discovery)
1. **behavioral_grounding** - Behavioral output analysis
2. **layer_sweep** - Layer-by-layer R_V measurement
3. **head_analysis** - Attention head investigation
4. Multi-seed parameter sweeps

### Historical (archive)
1. Failed runs (error.txt present)
2. Early explorations superseded by better methodology
3. Debugging runs
4. Incomplete runs (no summary.json)

## Steps

1. Extract experiment type and status from each summary.json
2. Check for error.txt files (failed runs)
3. Identify highest-quality run per experiment type
4. Create RESULTS_AUDIT.md with categorization decisions

## Artifacts
- `.planning/phases/04-results-organization/RESULTS_AUDIT.md`

## Verification
- Every run has a categorization decision
- Canonical runs have clear justification
- Failed runs identified and marked for archive
