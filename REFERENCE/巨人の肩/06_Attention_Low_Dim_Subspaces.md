# Attention Layers Add Into Low-Dimensional Subspaces â€“ Wang et al., 2025

- **Title**: Attention Layers Add Into Low-Dimensional Subspaces
- **Why relevant**: Shows attention outputs live in low-rank subspaces; provides geometric grounding for your R_V contraction observations.
- **Key methods to adapt**:
  - Intrinsic dimension / effective rank estimation for attention outputs.
  - Layer-wise dimensionality profiles.
- **Notes**: Use as theoretical backbone for interpreting R_V < 1 effects.
