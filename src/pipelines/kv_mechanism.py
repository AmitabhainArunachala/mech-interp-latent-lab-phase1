"""
KV Mechanism Pipeline (Gold Standard Pipeline 8)

Tests if the geometric contraction ($R_V < 1.0$) is stored in the KV Cache (Memory).
Does swapping the KV cache from a recursive prompt into a baseline prompt *transfer* the contracted geometry?

This measures GEOMETRY transfer, not just behavior.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import torch
from tqdm import tqdm
from transformers import DynamicCache

from prompts.loader import PromptLoader
from src.core.models import load_model, set_seed
from src.core.hooks import capture_v_projection
from src.metrics.rv import participation_ratio
from src.pipelines.registry import ExperimentResult


def _extract_full_kv_cache(model, tokenizer, prompt: str, device: str) -> DynamicCache:
    """Extract full KV cache for a prompt."""
    inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to(device)
    with torch.no_grad():
        out = model(**inputs, use_cache=True)
        return out.past_key_values


def _measure_rv_with_kv(
    model,
    input_ids: torch.Tensor,
    past_key_values: DynamicCache,
    early_layer: int,
    late_layer: int,
    window: int,
) -> float:
    """
    Measure R_V given a specific KV cache state.
    We run a forward pass for ONE new token (dummy) to measure the V-projection
    that *would* be generated given this history.
    """
    # Use the last token of input_ids as the "current" token to generate next from
    # Actually, we need to be careful. The V-projection happens *during* the processing 
    # of the current token, attending to the past.
    
    # We pass the last token of the prompt as the input, with the REST of the prompt in past_key_values.
    # Wait, past_key_values usually includes the full history. 
    # If we pass input_ids[:, -1:], we are generating the *next* token.
    # The V-projection at Layer L will attend to the *full* history (past + current).
    
    current_token = input_ids[:, -1:]
    
    with torch.no_grad():
        with capture_v_projection(model, early_layer) as v_early_storage:
            with capture_v_projection(model, late_layer) as v_late_storage:
                # Run forward pass for a single step
                model(current_token, past_key_values=past_key_values, use_cache=True)
                
        v_early = v_early_storage.get("v")
        v_late = v_late_storage.get("v")
        
        if v_early is None or v_late is None:
            return float('nan')
            
        # For a single token step, V is (1, 1, Dim).
        # We cannot measure PR on a single token (rank is always 1).
        # CRITICAL REALIZATION: R_V is a property of the *subspace* (Window > 1).
        # We cannot measure R_V on a single token V-vector.
        
        # WE MUST RECONSTRUCT THE WINDOW.
        # But we can't reconstruct the V-vectors for the *past* tokens without re-running them.
        # And if we re-run them with a *swapped* KV cache... that doesn't make sense. 
        # You can't run a forward pass "on top of" a KV cache for tokens that are already in the cache.
        
        # SOLUTION: We interpret "Geometry Transfer" as:
        # "Does the NEW token generated by this KV cache lie in a contracted subspace?"
        # To test this, we must generate a WINDOW of new tokens using the swapped KV.
        # Then measure R_V on *those* generated tokens.
        
        pass

    return float('nan')


def _patch_kv_selective(
    kv_source: DynamicCache,
    kv_target: DynamicCache,
    layer_range: Optional[Tuple[int, int]] = None,
) -> DynamicCache:
    """
    Selectively patch KV cache: only swap layers in layer_range.
    
    Args:
        kv_source: Source KV cache (from recursive prompt)
        kv_target: Target KV cache (from baseline prompt)
        layer_range: (start_layer, end_layer) to patch. If None, patch all layers.
    
    Returns:
        Patched KV cache (target with source layers in range)
    """
    if layer_range is None:
        # Patch all layers (original behavior)
        return kv_source
    
    start_layer, end_layer = layer_range
    num_layers = len(kv_target)
    
    # DynamicCache is a tuple of (key, value) tuples per layer
    # We need to create a new DynamicCache with selective patching
    patched_kv_list = []
    for layer_idx in range(num_layers):
        if start_layer <= layer_idx < end_layer:
            # Use source KV for this layer
            patched_kv_list.append(kv_source[layer_idx])
        else:
            # Use target KV for this layer
            patched_kv_list.append(kv_target[layer_idx])
    
    # Create new DynamicCache from patched list
    # DynamicCache constructor takes a list/tuple of (key, value) tuples
    return DynamicCache(patched_kv_list)


def _generate_window_and_measure_rv(
    model,
    tokenizer,
    prompt_ids: torch.Tensor,
    past_key_values: DynamicCache,
    window: int,
    early_layer: int,
    late_layer: int,
    device: str,
) -> float:
    """
    Generate `window` new tokens using the provided KV cache.
    Then measure R_V on *just those new tokens*.
    """
    generated_vs_early = []
    generated_vs_late = []
    
    current_ids = prompt_ids[:, -1:]
    current_kv = past_key_values
    
    for _ in range(window):
        with torch.no_grad():
            with capture_v_projection(model, early_layer) as v_early_store:
                with capture_v_projection(model, late_layer) as v_late_store:
                    out = model(current_ids, past_key_values=current_kv, use_cache=True)
            
            # Collect V-vectors
            ve = v_early_store.get("v")
            vl = v_late_store.get("v")
            
            if ve is not None: generated_vs_early.append(ve.detach())
            if vl is not None: generated_vs_late.append(vl.detach())
            
            # Greedy decode next token
            logits = out.logits[:, -1, :]
            next_token = torch.argmax(logits, dim=-1).unsqueeze(0)
            
            # Update loop vars
            current_ids = next_token
            current_kv = out.past_key_values
            
    if len(generated_vs_early) < window:
        return float('nan')
        
    # Stack to create (Window, Dim) tensors
    # generated_vs items are (1, 1, Dim) or (1, Dim)
    
    def stack_vs(v_list):
        # Normalize to (1, Dim)
        norm = [v[0, 0] if v.dim() == 3 else v[0] for v in v_list]
        return torch.stack(norm, dim=0) # (Window, Dim)
        
    tensor_early = stack_vs(generated_vs_early)
    tensor_late = stack_vs(generated_vs_late)
    
    pr_early = participation_ratio(tensor_early, window)
    pr_late = participation_ratio(tensor_late, window)
    
    if pr_early == 0 or np.isnan(pr_early) or np.isnan(pr_late):
        return float('nan')
        
    return float(pr_late / pr_early)


def run_kv_mechanism_from_config(cfg: Dict[str, Any], run_dir: Path) -> ExperimentResult:
    """Run KV Mechanism validation (Geometry Transfer)."""
    model_cfg = cfg.get("model") or {}
    params = cfg.get("params") or {}
    
    seed = int(cfg.get("seed") or 42)
    device = str(model_cfg.get("device") or ("cuda" if torch.cuda.is_available() else "cpu"))
    model_name = str(model_cfg.get("name") or "mistralai/Mistral-7B-v0.1")
    
    n_pairs = int(params.get("n_pairs") or 20)
    early_layer = int(params.get("early_layer") or 5)
    late_layer = int(params.get("late_layer") or 27)
    window = int(params.get("window") or 16)
    kv_layer_range = params.get("kv_layer_range")  # Optional: [start, end] layer range
    
    set_seed(seed)
    model, tokenizer = load_model(model_name, device=device)
    
    loader = PromptLoader()
    bank_version = loader.version
    (run_dir / "prompt_bank_version.txt").write_text(bank_version)
    (run_dir / "prompt_bank_version.json").write_text(
        json.dumps({"version": bank_version}, indent=2) + "\n"
    )
    
    # Select length-matched pairs (reusing logic from kv_sufficiency_matrix idea)
    # We need strict length matching for KV swap to be valid in most implementations
    # (though DynamicCache handles mismatched lengths better, the positional embeddings might mismatch)
    
    # For simplicity, we filter for pairs that accidentally match length or force it.
    # Better: Use the PromptLoader's balanced pairs and filter/truncate.
    
    raw_pairs = loader.get_balanced_pairs(n_pairs=n_pairs*2, seed=seed)
    pairs = []
    
    for rec_text, base_text in raw_pairs:
        # Naive length matching: truncate both to minimum length
        r_ids = tokenizer.encode(rec_text, add_special_tokens=False)
        b_ids = tokenizer.encode(base_text, add_special_tokens=False)
        
        common_len = min(len(r_ids), len(b_ids))
        if common_len < window: continue
        
        # Truncate to match exactly
        rec_ids = torch.tensor([r_ids[:common_len]], device=device)
        base_ids = torch.tensor([b_ids[:common_len]], device=device)
        
        pairs.append((rec_text, base_text, rec_ids, base_ids))
        if len(pairs) >= n_pairs: break
        
    print(f"Loaded {len(pairs)} length-matched pairs.")
    
    results = []
    
    for i, (rec_text, base_text, rec_ids, base_ids) in enumerate(tqdm(pairs)):
        try:
            # 1. Extract Recursive KV
            with torch.no_grad():
                out_rec = model(rec_ids, use_cache=True)
                rec_kv = out_rec.past_key_values
            
            # 2. Extract Baseline KV
            with torch.no_grad():
                out_base = model(base_ids, use_cache=True)
                base_kv = out_base.past_key_values
                
            # 3. Measure R_V of Recursive Generation (Control Positive)
            # Generate 16 tokens continuing from recursive prompt + recursive KV
            rv_rec = _generate_window_and_measure_rv(
                model, tokenizer, rec_ids, rec_kv, window, early_layer, late_layer, device
            )
            
            # 4. Measure R_V of Baseline Generation (Control Negative)
            rv_base = _generate_window_and_measure_rv(
                model, tokenizer, base_ids, base_kv, window, early_layer, late_layer, device
            )
            
            # 5. Measure R_V of Swapped Generation (The Test)
            # Prompt: Baseline. KV: Selectively patched (recursive KV in layer_range, baseline elsewhere)
            # Does the generation contract?
            if kv_layer_range:
                # Selective patching: only swap KV for layers in range
                patched_kv = _patch_kv_selective(rec_kv, base_kv, tuple(kv_layer_range))
            else:
                # Full swap: use recursive KV entirely (original behavior)
                patched_kv = rec_kv
            
            rv_swap = _generate_window_and_measure_rv(
                model, tokenizer, base_ids, patched_kv, window, early_layer, late_layer, device
            )
            
            results.append({
                "pair_idx": i,
                "rv_rec": rv_rec,
                "rv_base": rv_base,
                "rv_swap": rv_swap,
                "gap": rv_base - rv_rec,
                "restored": rv_base - rv_swap,
                "kv_layer_range": str(kv_layer_range) if kv_layer_range else "all",
            })
            
        except Exception as e:
            print(f"Error on pair {i}: {e}")
            continue
            
    # Save CSV
    df = pd.DataFrame(results)
    out_csv = run_dir / "kv_mechanism.csv"
    df.to_csv(out_csv, index=False)
    
    # Stats
    summary = {
        "experiment": "kv_mechanism",
        "model_name": model_name,
        "n_pairs": len(df),
        "mean_rv_rec": float(df["rv_rec"].mean()),
        "mean_rv_base": float(df["rv_base"].mean()),
        "mean_rv_swap": float(df["rv_swap"].mean()),
        "prompt_bank_version": bank_version,
        "kv_layer_range": kv_layer_range if kv_layer_range else "all",
        "artifacts": {"csv": str(out_csv)},
    }
    
    # Transfer %
    gap = summary["mean_rv_base"] - summary["mean_rv_rec"]
    restored = summary["mean_rv_base"] - summary["mean_rv_swap"]
    
    summary["transfer_efficiency"] = (restored / gap * 100.0) if gap > 1e-6 else 0.0
    
    return ExperimentResult(summary=summary)

