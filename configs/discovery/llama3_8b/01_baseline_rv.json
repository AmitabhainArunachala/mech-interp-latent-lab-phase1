{
  "experiment": "cross_architecture_validation",
  "description": "Phase 2: Baseline R_V separation test for Llama-3-8B",
  "params": {
    "model": "meta-llama/Meta-Llama-3-8B",
    "early_layer": 5,
    "late_layer": 27,
    "window": 16,
    "n_champions": 50,
    "n_controls": 50,
    "seed": 42
  },
  "success_criteria": {
    "rv_champions_max": 0.65,
    "rv_controls_min": 0.70,
    "separation_p_max": 0.01,
    "cohens_d_min": 1.0
  },
  "notes": "Llama-3-8B has 32 layers like Mistral. Late layer = 27 (84% depth)."
}
