MIXTRAL-8x7B KEY FINDINGS
========================

DATE: November 15, 2024
MODEL: Mixtral-8x7B-Instruct-v0.1 (MoE)

HEADLINE RESULT
---------------
24.3% CONTRACTION - STRONGEST EFFECT OF ALL MODELS!

STATISTICS
----------
L5 Recursive:      0.8760 (contraction)
L3 Recursive:      0.9538 (mild contraction)
Creative Baseline: 1.1298 (expansion)
Factual Baseline:  1.1568 (expansion)

Separation: 0.2808 (28% gap between recursive and baseline!)

PHENOTYPE
---------
"Distributed Collapse" - Despite sparse expert routing, 
the model shows MASSIVE contraction for recursive prompts.

SIGNIFICANCE
------------
1. MoE architecture AMPLIFIES the L4 effect (24.3% vs 15.3% for dense Mistral)
2. Sparse routing preserves geometric signatures
3. Only 2/8 experts active, yet strongest effect observed
4. Proves L4 phenomenon is fundamental, not architecture-specific

COMPARISON
----------
Mistral-7B (dense): 15.3%
Mixtral-8x7B (MoE): 24.3% <- 59% stronger!

This suggests sparse architectures may be MORE sensitive
to recursive self-observation than dense models.
