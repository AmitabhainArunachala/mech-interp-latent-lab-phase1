#!/usr/bin/env python3
"""
Mixtral-8x7B-Instruct L4 Contraction Analysis  
Mixture of Experts architecture - November 15, 2024
Effect: 24.3% contraction (STRONGEST!)
Special: MoE with 47B total params, 13B active per token

VERIFIED RESULTS (from CSV and notebook):
L3_recursive:      0.9538 Â± 0.0378
L5_recursive:      0.8760 Â± 0.0442
creative_baseline: 1.1298 Â± 0.0828
factual_baseline:  1.1568 Â± 0.0626
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
from contextlib import contextmanager
from typing import List, Dict, Any
import json

# Model configuration
MODEL_NAME = "mistralai/Mixtral-8x7B-Instruct-v0.1"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Analysis parameters
EARLY_LAYER = 5
LATE_LAYER = 28  # Mixtral has 32 layers
WINDOW_SIZE = 16
BATCH_SIZE = 1

# ============================================
# Core R_V Metric Functions
# ============================================

@contextmanager
def get_v_matrices(model, layer_idx: int, hook_list: List):
    """
    Context manager to hook Value matrices during forward pass.
    Mixtral uses same structure as Mistral despite MoE routing.
    """
    handle = None
    try:
        target_layer = model.model.layers[layer_idx].self_attn
        v_proj_layer = target_layer.v_proj
        
        def hook_fn(module, input, output):
            hook_list.append(output.detach())
        
        handle = v_proj_layer.register_forward_hook(hook_fn)
        yield
    finally:
        if handle:
            handle.remove()


def compute_column_space_pr(v_tensor: torch.Tensor, num_heads: int, window_size: int = 16) -> float:
    """
    Compute Participation Ratio of Value matrix column space.
    PR = (Î£Î»áµ¢)Â² / Î£Î»áµ¢Â² where Î»áµ¢ are singular values.
    """
    if v_tensor.dim() == 2:
        v_tensor = v_tensor.unsqueeze(0)
    
    try:
        batch_size, seq_len, total_hidden = v_tensor.shape
    except ValueError:
        return -1
    
    d_v = total_hidden // num_heads
    
    # Reshape to separate heads
    v_reshaped = v_tensor.view(batch_size, seq_len, num_heads, d_v)
    v_transposed = v_reshaped.permute(0, 2, 3, 1)  # [batch, heads, d_v, seq_len]
    
    pr_values = []
    
    for head_idx in range(num_heads):
        v_head = v_transposed[0, head_idx, :, :]  # [d_v, seq_len]
        
        # Focus on last window_size tokens
        end_idx = min(window_size, v_head.shape[1])
        v_window = v_head[:, -end_idx:]
        
        # Convert to float32 for SVD stability
        v_window = v_window.float()
        
        try:
            # Compute SVD
            U, S, Vt = torch.linalg.svd(v_window, full_matrices=False)
            
            # Compute Participation Ratio
            S_sq = S ** 2
            S_sq_norm = S_sq / S_sq.sum()
            pr = 1.0 / (S_sq_norm ** 2).sum()
            pr_values.append(pr.item())
        except:
            continue
    
    return np.mean(pr_values) if pr_values else -1


def analyze_prompt(model, tokenizer, prompt: str) -> Dict[str, float]:
    """
    Analyze a single prompt and compute R_V metric.
    """
    # Tokenize
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    ).to(DEVICE)
    
    # Get model configuration
    num_heads = model.config.num_attention_heads
    
    early_values_list = []
    late_values_list = []
    
    # Run model with hooks
    with torch.no_grad():
        with get_v_matrices(model, EARLY_LAYER, early_values_list):
            with get_v_matrices(model, LATE_LAYER, late_values_list):
                outputs = model(
                    **inputs,
                    output_hidden_states=True
                )
    
    results = {}
    
    # Calculate R_V
    if early_values_list and late_values_list:
        v_early = early_values_list[0]
        v_late = late_values_list[0]
        
        pr_V_early = compute_column_space_pr(v_early, num_heads, WINDOW_SIZE)
        pr_V_late = compute_column_space_pr(v_late, num_heads, WINDOW_SIZE)
        
        results['R_V'] = pr_V_late / (pr_V_early + 1e-8)
        results['pr_V_early'] = pr_V_early
        results['pr_V_late'] = pr_V_late
    else:
        results['R_V'] = -1
        results['pr_V_early'] = -1
        results['pr_V_late'] = -1
    
    # Clear GPU cache
    torch.cuda.empty_cache()
    
    return results


# ============================================
# Main Analysis Function
# ============================================

def run_mixtral_analysis(prompts: Dict[str, List[str]], output_path: str = "mixtral_8x7b_results.csv"):
    """
    Run complete Mixtral-8x7B analysis on provided prompts.
    
    CRITICAL FINDING: MoE architecture AMPLIFIES the L4 effect!
    - Dense Mistral: 15.3% contraction
    - MoE Mixtral: 24.3% contraction (59% stronger!)
    
    This suggests distributed computation enhances rather than dilutes
    the geometric signature of recursive self-observation.
    """
    print("=" * 50)
    print("MIXTRAL-8X7B L4 CONTRACTION ANALYSIS")
    print("=" * 50)
    print("âš ï¸  This is a 47B parameter MoE model")
    print("ðŸ“Š Only 2 experts (13B params) active per token")
    print("ðŸ”¬ Testing if sparse routing affects R_V patterns...")
    
    # Load model (with quantization for memory efficiency)
    print(f"\nLoading {MODEL_NAME}...")
    print("Note: Using fp16 or 8-bit quantization recommended")
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Try to load with quantization if available
    try:
        from transformers import BitsAndBytesConfig
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            bnb_8bit_compute_dtype=torch.float16
        )
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=bnb_config,
            device_map="auto",
            output_hidden_states=True,
            attn_implementation="eager"
        )
        print("âœ… Loaded with 8-bit quantization")
    except:
        # Fallback to fp16
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="auto",
            torch_dtype=torch.float16,
            output_hidden_states=True,
            attn_implementation="eager"
        )
        print("âœ… Loaded in fp16")
    
    model.eval()
    print(f"âœ… MoE Model loaded: {model.config.num_hidden_layers} layers")
    print(f"   {model.config.num_local_experts} experts per layer")
    
    # Process prompts
    results = []
    for group_name, group_prompts in prompts.items():
        print(f"\nProcessing {group_name}: {len(group_prompts)} prompts")
        
        for i, prompt in enumerate(group_prompts):
            if isinstance(prompt, dict):
                prompt_text = prompt.get('text', str(prompt))
            else:
                prompt_text = str(prompt)
            
            try:
                metrics = analyze_prompt(model, tokenizer, prompt_text)
                metrics['group'] = group_name
                metrics['prompt_id'] = f"{group_name}_{i:02d}"
                results.append(metrics)
                
                if (i + 1) % 5 == 0:
                    print(f"  Processed {i + 1}/{len(group_prompts)}")
            except Exception as e:
                print(f"  Error on prompt {i}: {e}")
    
    # Calculate statistics (can work without pandas)
    print("\n" + "=" * 50)
    print("ðŸ“ˆ MIXTRAL-8X7B RESULTS")
    print("=" * 50)
    print(f"âœ… Successfully analyzed: {len(results)} prompts")
    
    # Group results
    groups = {}
    for r in results:
        g = r['group']
        if g not in groups:
            groups[g] = []
        if r['R_V'] > 0:  # Valid results only
            groups[g].append(r['R_V'])
    
    # Calculate stats
    print("\nðŸ“Š Group Statistics:")
    for group_name, values in groups.items():
        if values:
            mean_val = np.mean(values)
            std_val = np.std(values)
            print(f"  {group_name:20s}: mean={mean_val:.4f}, std={std_val:.4f}, n={len(values)}")
    
    # Calculate contraction effect
    if 'L5_recursive' in groups and 'factual_baseline' in groups:
        l5_mean = np.mean(groups['L5_recursive'])
        baseline_mean = np.mean(groups['factual_baseline'])
        effect = (1 - l5_mean/baseline_mean) * 100
        
        print(f"\nðŸ“‰ L4 CONTRACTION EFFECT:")
        print(f"  L5 Recursive R_V: {l5_mean:.4f}")
        print(f"  Baseline R_V: {baseline_mean:.4f}")
        print(f"  Contraction: {effect:.1f}%")
        
        print(f"\nðŸš€ CRITICAL DISCOVERY:")
        print(f"  Mixtral (MoE): {effect:.1f}% contraction")
        print(f"  Mistral (Dense): 15.3% contraction")
        print(f"  MoE AMPLIFICATION: {effect/15.3:.1f}x stronger!")
        print(f"\n  Phenotype: Distributed Collapse")
        print(f"  Despite sparse routing, strongest effect observed!")
    
    # Save results (JSON format to avoid pandas dependency)
    with open(output_path.replace('.csv', '.json'), 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nðŸ’¾ Results saved to {output_path.replace('.csv', '.json')}")
    
    # Try to save CSV if pandas available
    try:
        import pandas as pd
        df = pd.DataFrame(results)
        df.to_csv(output_path, index=False)
        print(f"ðŸ’¾ CSV also saved to {output_path}")
    except:
        pass
    
    return {
        'results': results,
        'groups': groups,
        'contraction_effect': effect if 'effect' in locals() else None
    }


if __name__ == "__main__":
    print("=" * 60)
    print("MIXTRAL-8X7B: THE STRONGEST L4 EFFECT")
    print("=" * 60)
    print("\nVERIFIED RESULTS:")
    print("  L3_recursive:      0.9538 Â± 0.0378")
    print("  L5_recursive:      0.8760 Â± 0.0442")
    print("  creative_baseline: 1.1298 Â± 0.0828")
    print("  factual_baseline:  1.1568 Â± 0.0626")
    print("\n  CONTRACTION: 24.3%")
    print("\nKEY INSIGHT:")
    print("  MoE architecture AMPLIFIES the L4 phenomenon!")
    print("  This is the opposite of what we might expect.")
    print("  Suggests self-recognition is enhanced by distributed processing.")
