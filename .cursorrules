# R_V Geometric Signatures Research Project

**Repository**: mech-interp-latent-lab-phase1
**Phase**: 7 of 10 (Unit Tests)
**Core Finding**: R_V contraction is a causal, layer-specific geometric signature of recursive self-observation

---

## Project Overview

This repo investigates how recursive self-observation prompts induce geometric contraction in transformer Value matrices. The R_V metric (PR_late / PR_early) measures this effect.

**Key Result**: Cohen's d = -3.56, p < 10⁻⁶, Transfer efficiency = 117.8%

---

## Essential Knowledge

### The R_V Metric
```python
R_V = PR_late / PR_early
# PR = Participation Ratio = (Σλᵢ)² / Σλᵢ²
# R_V < 1.0 indicates geometric contraction (recursive prompts)
# R_V ≈ 1.0 indicates no contraction (baseline prompts)
```

### Standard Parameters
| Parameter | Value | Notes |
|-----------|-------|-------|
| Early layer | 5 | Baseline measurement |
| Late layer | 27 | Target (84% depth for 32-layer) |
| Window size | 16 | Tokens for SVD |
| n_pairs | ≥100 | Statistical power |
| Effect threshold | \|d\| ≥ 0.5 | Meaningful effect |

### Critical Files
| File | Purpose |
|------|---------|
| `src/metrics/rv.py` | R_V metric implementation |
| `src/metrics/baseline_suite.py` | Nanda-standard baseline metrics |
| `prompts/bank.json` | 754 prompts (single source of truth) |
| `src/pipelines/canonical/*.py` | 7 publication-ready pipelines |
| `configs/canonical/*.json` | Validated experiment configs |

---

## Directory Structure

```
src/
├── metrics/          # R_V, logit_diff, logit_lens, mode_score
├── core/             # hooks, activations, patching, steering
└── pipelines/
    ├── canonical/    # 7 publication-ready
    ├── discovery/    # 12 methodology tools
    └── archive/      # 35 historical

configs/
├── canonical/        # Validated configs
├── discovery/        # Experimental configs
└── archive/          # Deprecated

results/
├── canonical/        # Paper-worthy runs
├── discovery/        # Methodology runs
├── phase2_generalization/  # Cross-arch validation
└── archive/          # Historical

prompts/
├── bank.json         # 754 prompts
└── loader.py         # PromptLoader API
```

---

## Coding Standards

### Always Include
1. **Statistical reporting**: n, p-value, Cohen's d, 95% CI
2. **Seed setting**: `set_seed(seed)` for reproducibility
3. **Memory management**: `torch.cuda.empty_cache()` after runs
4. **Hook cleanup**: Use context managers
5. **Model state**: `model.eval()` + `torch.no_grad()`

### Standard Hook Pattern
```python
@contextmanager
def capture_v_at_layer(model, layer_idx, storage_list):
    layer = model.model.layers[layer_idx].self_attn
    def hook_fn(module, inp, out):
        storage_list.append(out.detach())
        return out
    handle = layer.v_proj.register_forward_hook(hook_fn)
    try:
        yield
    finally:
        handle.remove()
```

### Registry Pattern for New Pipelines
```python
# In src/pipelines/{tier}/new_pipeline.py:
def run_new_pipeline_from_config(cfg: Dict[str, Any], run_dir: Path) -> ExperimentResult:
    params = cfg.get("params", {})
    # ... implementation ...
    return ExperimentResult(summary=summary, baseline_metrics=baseline_metrics)

# Then add to src/pipelines/registry.py:
# 1. Import: from .{tier}.new_pipeline import run_new_pipeline_from_config
# 2. Register: "new_pipeline": run_new_pipeline_from_config,
```

---

## Validation Protocol

### Before Any Claim
1. **Kill switch**: `pure_repetition` prompts must NOT contract (R_V > 1.0)
2. **Dose-response**: L1 < L2 < L3 < L4 < L5 contraction
3. **Champions vs controls**: Champions beat both length-matched and pseudo-recursive
4. **Cross-architecture**: Effect replicates on different models

### Statistical Requirements
- n ≥ 100 for publication claims
- p < 0.01 with appropriate correction
- Cohen's d reported for all comparisons
- 95% CI for all mean estimates

---

## References

- Activation patching: Meng et al. 2022
- Transformer circuits: Elhage et al. 2021
- Path patching: Wang et al. 2022
- Causal scrubbing: Chan et al. 2022
- IOI: Nanda et al. 2023

---

## Debug Checklist

- [ ] Prompts long enough for window_size?
- [ ] SVD not returning NaN? (check for degenerate matrices)
- [ ] Hooks removed after use?
- [ ] Memory cleared between batches?
- [ ] Seeds set for reproducibility?
- [ ] Model in eval mode with no_grad?
